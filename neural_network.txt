00:04
This is a three. It's sloppily written and rendered at an extremely low resolution of 28 by 28 pixels.
00:10
But your brain has no trouble recognizing it as a three and I want you to take a moment to appreciate
00:15
How crazy it is that brains can do this so effortlessly?
00:18
I mean this this and this are also recognizable as threes,
00:23
even though the specific values of each pixel is very different from one image to the next.
00:28
The particular light-sensitive cells in your eye that are firing when you see this three
00:33
are very different from the ones firing when you see this three.
00:37
But something in that crazy smart visual cortex of yours
00:41
resolves these as representing the same idea while at the same time recognizing other images as their own distinct ideas
00:48
But if I told you hey sit down and write for me a program that takes in a grid of 28 by 28
00:55
pixels like this and outputs a single number between 0 and 10 telling you what it thinks the digit is
01:02
Well the task goes from comically trivial to dauntingly difficult
01:06
Unless you've been living under a rock
01:08
I think I hardly need to motivate the relevance and importance of machine learning and neural networks to the present into the future
01:14
But what I want to do here is show you what a neural network actually is
01:18
Assuming no background and to help visualize what it's doing not as a buzzword but as a piece of math
01:24
My hope is just that you come away feeling like this structure itself is
01:28
Motivated and to feel like you know what it means when you read or you hear about a neural network quote-unquote learning
01:34
This video is just going to be devoted to the structure component of that and the following one is going to tackle learning
01:40
What we're going to do is put together a neural network that can learn to recognize handwritten digits
01:49
This is a somewhat classic example for
01:51
Introducing the topic and I'm happy to stick with the status quo here because at the end of the two videos I want to point
01:56
You to a couple good resources where you can learn more and where you can download the code that does this and play with it?
02:02
on your own computer
02:04
There are many many variants of neural networks and in recent years
02:08
There's been sort of a boom in research towards these variants
02:12
But in these two introductory videos you and I are just going to look at the simplest plain-vanilla form with no added frills
02:19
This is kind of a necessary
02:21
prerequisite for understanding any of the more powerful modern variants and
02:24
Trust me it still has plenty of complexity for us to wrap our minds around
02:28
But even in this simplest form it can learn to recognize handwritten digits
02:32
Which is a pretty cool thing for a computer to be able to do.
02:37
And at the same time you'll see how it does fall short of a couple hopes that we might have for it
02:43
As the name suggests neural networks are inspired by the brain, but let's break that down
02:48
What are the neurons and in what sense are they linked together?
02:52
Right now when I say neuron all I want you to think about is a thing that holds a number
02:58
Specifically a number between 0 & 1 it's really not more than that
03:03
For example the network starts with a bunch of neurons corresponding to each of the 28 times 28 pixels of the input image
03:11
which is
03:12
784 neurons in total each one of these holds a number that represents the grayscale value of the corresponding pixel
03:20
ranging from 0 for black pixels up to 1 for white pixels
03:24
This number inside the neuron is called its activation and the image you might have in mind here
03:30
Is that each neuron is lit up when its activation is a high number?
03:36
So all of these 784 neurons make up the first layer of our network
03:45
Now jumping over to the last layer this has ten neurons each representing one of the digits
03:51
the activation in these neurons again some number that's between zero and one
03:56
Represents how much the system thinks that a given image?
04:00
Corresponds with a given digit. There's also a couple layers in between called the hidden layers
04:06
Which for the time being?
04:07
Should just be a giant question mark for how on earth this process of recognizing digits is going to be handled
04:13
In this network I chose two hidden layers each one with 16 neurons and admittedly that's kind of an arbitrary choice
04:20
to be honest I chose two layers based on how I want to motivate the structure in just a moment and
04:25
16 well that was just a nice number to fit on the screen in practice
04:29
There is a lot of room for experiment with a specific structure here
04:32
The way the network operates activations in one layer determine the activations of the next layer
04:38
And of course the heart of the network as an information processing mechanism comes down to exactly how those
04:45
activations from one layer bring about activations in the next layer
04:48
It's meant to be loosely analogous to how in biological networks of neurons some groups of neurons firing
04:55
cause certain others to fire
04:57
Now the network
04:58
I'm showing here has already been trained to recognize digits and let me show you what I mean by that
05:03
It means if you feed in an image lighting up all
05:06
784 neurons of the input layer according to the brightness of each pixel in the image
05:12
That pattern of activations causes some very specific pattern in the next layer
05:17
Which causes some pattern in the one after it?
05:19
Which finally gives some pattern in the output layer and?
05:22
The brightest neuron of that output layer is the network's choice so to speak for what digit this image represents?
05:32
And before jumping into the math for how one layer influences the next or how training works?
05:37
Let's just talk about why it's even reasonable to expect a layered structure like this to behave intelligently
05:43
What are we expecting here? What is the best hope for what those middle layers might be doing?
05:48
Well when you or I recognize digits we piece together various components a nine has a loop up top and a line on the right
05:57
an 8 also has a loop up top, but it's paired with another loop down low
06:02
A 4 basically breaks down into three specific lines and things like that
06:07
Now in a perfect world we might hope that each neuron in the second-to-last layer
06:12
corresponds with one of these sub components
06:14
That anytime you feed in an image with say a loop up top like a 9 or an 8
06:19
There's some specific
06:21
Neuron whose activation is going to be close to one and I don't mean this specific loop of pixels the hope would be that any
06:28
Generally loopy pattern towards the top sets off this neuron that way going from the third layer to the last one
06:35
just requires learning which combination of sub components corresponds to which digits
06:40
Of course that just kicks the problem down the road
06:42
Because how would you recognize these sub components or even learn what the right sub components should be and I still haven't even talked about
06:49
How one layer influences the next but run with me on this one for a moment
06:53
recognizing a loop can also break down into subproblems
06:56
One reasonable way to do this would be to first recognize the various little edges that make it up
07:03
Similarly a long line like the kind you might see in the digits 1 or 4 or 7
07:08
Well that's really just a long edge or maybe you think of it as a certain pattern of several smaller edges
07:14
So maybe our hope is that each neuron in the second layer of the network
07:20
corresponds with the various relevant little edges
07:23
Maybe when an image like this one comes in it lights up all of the neurons
07:28
associated with around eight to ten specific little edges
07:31
which in turn lights up the neurons associated with the upper loop and a long vertical line and
07:37
Those light up the neuron associated with a nine
07:40
whether or not
07:41
This is what our final network actually does is another question, one that I'll come back to once we see how to train the network
07:47
But this is a hope that we might have. A sort of goal with the layered structure like this
07:53
Moreover you can imagine how being able to detect edges and patterns like this would be really useful for other image recognition tasks
07:59
And even beyond image recognition there are all sorts of intelligent things you might want to do that break down into layers of abstraction
08:07
Parsing speech for example involves taking raw audio and picking out distinct sounds which combine to make certain syllables
08:15
Which combine to form words which combine to make up phrases and more abstract thoughts etc
08:20
But getting back to how any of this actually works picture yourself right now designing
08:25
How exactly the activations in one layer might determine the activations in the next?
08:30
The goal is to have some mechanism that could conceivably combine pixels into edges
08:35
Or edges into patterns or patterns into digits and to zoom in on one very specific example
08:41
Let's say the hope is for one particular
08:44
Neuron in the second layer to pick up on whether or not the image has an edge in this region here
08:50
The question at hand is what parameters should the network have
08:55
what dials and knobs should you be able to tweak so that it's expressive enough to potentially capture this pattern or
09:02
Any other pixel pattern or the pattern that several edges can make a loop and other such things?
09:08
Well, what we'll do is assign a weight to each one of the connections between our neuron and the neurons from the first layer
09:15
These weights are just numbers
09:18
then take all those activations from the first layer and compute their weighted sum according to these weights I
09:27
Find it helpful to think of these weights as being organized into a little grid of their own
09:31
And I'm going to use green pixels to indicate positive weights and red pixels to indicate negative weights
09:37
Where the brightness of that pixel is some loose depiction of the weights value?
09:42
Now if we made the weights associated with almost all of the pixels zero
09:46
except for some positive weights in this region that we care about
09:49
then taking the weighted sum of
09:51
all the pixel values really just amounts to adding up the values of the pixel just in the region that we care about
09:58
And, if you really want it to pick up on whether there's an edge here what you might do is have some negative weights
10:04
associated with the surrounding pixels
10:07
Then the sum is largest when those middle pixels are bright, but the surrounding pixels are darker
10:14
When you compute a weighted sum like this you might come out with any number
10:18
but for this network what we want is for activations to be some value between 0 & 1
10:23
so a common thing to do is to pump this weighted sum
10:26
Into some function that squishes the real number line into the range between 0 & 1 and
10:32
A common function that does this is called the sigmoid function also known as a logistic curve
10:37
basically very negative inputs end up close to zero very positive inputs end up close to 1
10:43
and it just steadily increases around the input 0
10:49
So the activation of the neuron here is basically a measure of how positive the relevant weighted sum is
10:57
But maybe it's not that you want the neuron to light up when the weighted sum is bigger than 0
11:02
Maybe you only want it to be active when the sum is bigger than say 10
11:06
That is you want some bias for it to be inactive
11:10
what we'll do then is just add in some other number like negative 10 to this weighted sum
11:16
Before plugging it through the sigmoid squishification function
11:20
That additional number is called the bias
11:23
So the weights tell you what pixel pattern this neuron in the second layer is picking up on and the bias
11:29
tells you how high the weighted sum needs to be before the neuron starts getting meaningfully active
11:35
And that is just one neuron
11:38
Every other neuron in this layer is going to be connected to all
11:42
784 pixels neurons from the first layer and each one of those 784 connections has its own weight associated with it
11:51
also each one has some bias some other number that you add on to the weighted sum before squishing it with the sigmoid and
11:58
That's a lot to think about with this hidden layer of 16 neurons
12:02
that's a total of 784 times 16 weights along with 16 biases
12:08
And all of that is just the connections from the first layer to the second the connections between the other layers
12:14
Also, have a bunch of weights and biases associated with them
12:17
All said and done this network has almost exactly
12:21
13,000 total weights and biases
12:24
13,000 knobs and dials that can be tweaked and turned to make this network behave in different ways
12:30
So when we talk about learning?
12:32
What that's referring to is getting the computer to find a valid setting for all of these many many numbers so that it'll actually solve
12:40
the problem at hand
12:42
one thought
12:43
Experiment that is at once fun and kind of horrifying is to imagine sitting down and setting all of these weights and biases by hand
12:50
Purposefully tweaking the numbers so that the second layer picks up on edges the third layer picks up on patterns etc
12:56
I personally find this satisfying rather than just reading the network as a total black box
13:01
Because when the network doesn't perform the way you
13:04
anticipate if you've built up a little bit of a relationship with what those weights and biases actually mean you have a starting place for
13:11
Experimenting with how to change the structure to improve or when the network does work?
13:16
But not for the reasons you might expect
13:18
Digging into what the weights and biases are doing is a good way to challenge your assumptions and really expose the full space of possible
13:25
solutions
13:26
By the way the actual function here is a little cumbersome to write down. Don't you think?
13:32
So let me show you a more notationally compact way that these connections are represented. This is how you'd see it
13:38
If you choose to read up more about neural networks
13:41
Organize all of the activations from one layer into a column as a vector
13:47
Then organize all of the weights as a matrix where each row of that matrix
13:52
corresponds to the connections between one layer and a particular neuron in the next layer
13:58
What that means is that taking the weighted sum of the activations in the first layer according to these weights?
14:04
Corresponds to one of the terms in the matrix vector product of everything we have on the left here
14:13
By the way so much of machine learning just comes down to having a good grasp of linear algebra
14:18
So for any of you who want a nice visual understanding for matrices and what matrix vector multiplication means take a look at the series I did on linear algebra
14:27
especially chapter three
14:28
Back to our expression instead of talking about adding the bias to each one of these values independently we represent it by
14:36
Organizing all those biases into a vector and adding the entire vector to the previous matrix vector product
14:42
Then as a final step
14:44
I'll rap a sigmoid around the outside here
14:47
And what that's supposed to represent is that you're going to apply the sigmoid function to each specific
14:52
component of the resulting vector inside
14:55
So once you write down this weight matrix and these vectors as their own symbols you can
15:01
communicate the full transition of activations from one layer to the next in an extremely tight and neat little expression and
15:07
This makes the relevant code both a lot simpler and a lot faster since many libraries optimize the heck out of matrix multiplication
15:17
Remember how earlier I said these neurons are simply things that hold numbers
15:21
Well of course the specific numbers that they hold depends on the image you feed in
15:27
So it's actually more accurate to think of each neuron as a function one that takes in the
15:33
outputs of all the neurons in the previous layer and spits out a number between zero and one
15:38
Really the entire network is just a function one that takes in
15:42
784 numbers as an input and spits out ten numbers as an output
15:47
It's an absurdly
15:48
Complicated function one that involves thirteen thousand parameters in the forms of these weights and biases that pick up on certain patterns and which involves
15:56
iterating many matrix vector products and the sigmoid squish evocation function
16:00
But it's just a function nonetheless and in a way it's kind of reassuring that it looks complicated
16:06
I mean if it were any simpler what hope would we have that it could take on the challenge of recognizing digits?
16:12
And how does it take on that challenge? How does this network learn the appropriate weights and biases just by looking at data? Oh?
16:20
That's what I'll show in the next video, and I'll also dig a little more into what this particular network we are seeing is really doing
16:27
Now is the point I suppose I should say subscribe to stay notified about when that video or any new videos come out
16:32
But realistically most of you don't actually receive notifications from YouTube, do you ?
16:37
Maybe more honestly I should say subscribe so that the neural networks that underlie YouTube's
16:42
Recommendation algorithm are primed to believe that you want to see content from this channel get recommended to you
16:48
anyway stay posted for more
16:50
Thank you very much to everyone supporting these videos on patreon
16:53
I've been a little slow to progress in the probability series this summer
16:56
But I'm jumping back into it after this project so patrons you can look out for updates there
17:03
To close things off here I have with me Lisha Li
17:05
Lee who did her PhD work on the theoretical side of deep learning and who currently works at a venture capital firm called amplify partners
17:12
Who kindly provided some of the funding for this video so Lisha one thing
17:16
I think we should quickly bring up is this sigmoid function
17:19
As I understand it early networks used this to squish the relevant weighted sum into that interval between zero and one
17:24
You know kind of motivated by this biological analogy of neurons either being inactive or active (Lisha) - Exactly
17:30
(3B1B) - But relatively few modern networks actually use sigmoid anymore. That's kind of old school right ? (Lisha) - Yeah or rather
17:36
ReLU seems to be much easier to train (3B1B) - And ReLU really stands for rectified linear unit
17:42
(Lisha) - Yes it's this kind of function where you're just taking a max of 0 and a where a is given by
17:49
what you were explaining in the video and what this was sort of motivated from I think was a
17:54
partially by a biological
17:56
Analogy with how
17:58
Neurons would either be activated or not and so if it passes a certain threshold
18:03
It would be the identity function
18:05
But if it did not then it would just not be activated so be zero so it's kind of a simplification
18:10
Using sigmoids didn't help training, or it was very difficult to train
18:14
It's at some point and people just tried relu and it happened to work
18:20
Very well for these incredibly
18:22
Deep neural networks. (3B1B) - All right
18:25
Thank You Lisha
18:26
for background amplify partners in early-stage VC invests in technical founders building the next generation of companies focused on the
18:33
applications of AI if you or someone that you know has ever thought about starting a company someday
18:38
Or if you're working on an early-stage one right now the Amplify folks would love to hear from you
18:43
they even set up a specific email for this video 3blue1brown@amplifypartners.com
18:48
so feel free to reach out to them through that